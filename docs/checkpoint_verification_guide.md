# ğŸ” Checkpoint Verification Tools - Quick Start Guide

## Overview

This directory contains comprehensive checkpoint verification and validation tools to ensure training safety and catch corrupted checkpoints early.

---

## ğŸš€ Quick Usage

### Before Resuming Training

**Always run verification first:**

```bash
# Basic verification (recommended)
python scripts/verify_checkpoint.py --checkpoint_path /path/to/checkpoint

# Strict mode (treat warnings as errors)
python scripts/verify_checkpoint.py --checkpoint_path /path/to/checkpoint --strict
```

### Verification Tools Comparison

| Tool | Purpose | Use Case | Speed |
|------|---------|----------|-------|
| `verify_checkpoint.py` | **Comprehensive verification** | Pre-flight checks before resuming | ~2 seconds |
| `validate_checkpoint.py` | **State interpretation** | Understand checkpoint metadata | ~1 second |
| `quick_verify_checkpoint()` | **Fast integrity check** | Auto-integrated in training | < 1 second |

---

## ğŸ“‹ Tool Details

### 1. `verify_checkpoint.py` - Comprehensive Verification

**What it checks:**
- âœ… Directory and file existence
- âœ… File integrity (not empty, not corrupted)
- âœ… Training state consistency
- âœ… Optimizer state validation
- âœ… Metadata sanity checks

**Exit codes:**
- `0` = Verification passed
- `1` = Verification failed

**Example output:**
```
======================================================================
CHECKPOINT VERIFICATION REPORT
======================================================================

ğŸ” CHECK 1: Directory Existence
âœ… Directory exists: /path/to/checkpoint

ğŸ” CHECK 2: Required Files
âœ… training_state.pth: 12.45 MB
âœ… config.json: 0.00 MB
âœ… model.safetensors: 475.50 MB

ğŸ” CHECK 3: Training State Integrity
âœ… Training state loaded successfully
âœ… epoch=2
âœ… step=0
âœ… total_batches=1270

ğŸ” CHECK 4: Metadata Consistency
âœ… Epoch and step values are valid
âœ… End-of-epoch checkpoint (epoch 2 completed)

ğŸ” CHECK 5: Optimizer State
âœ… Optimizer has performed 2532 steps
âœ… Optimizer steps consistent with metadata

======================================================================
SUMMARY
======================================================================
âœ… Passed: 18
âš ï¸  Warnings: 0
âŒ Errors: 0

âœ… VERIFICATION PASSED
Checkpoint is valid and safe to resume training from.
```

---

### 2. `validate_checkpoint.py` - State Interpretation

**What it shows:**
- ğŸ“Š Checkpoint metadata (epoch, step, batches)
- ğŸ” Interpretation (completed vs incomplete)
- ğŸ”§ Optimizer state
- ğŸ“ Model files
- ğŸ“ Resume behavior prediction

**Usage:**
```bash
python scripts/validate_checkpoint.py --checkpoint_path /path/to/checkpoint
```

**Example output:**
```
======================================================================
CHECKPOINT VALIDATION
======================================================================

ğŸ“Š CHECKPOINT METADATA:
  Epoch: 2
  Step: 0
  Total Batches: 1270

ğŸ” INTERPRETATION:
  âœ… This is an END-OF-EPOCH checkpoint
  ğŸ“ Meaning: Epoch 2 is COMPLETED
  ğŸ“¦ Model has trained on ALL batches 0-1269
  â¡ï¸  When resuming: Will start epoch 3

======================================================================
SUMMARY
======================================================================
âœ… This checkpoint represents a COMPLETE epoch 2
âœ… Safe to resume - will start epoch 3
âœ… No batches will be skipped or duplicated
```

---

### 3. `quick_verify_checkpoint()` - Integrated Check

**Automatically runs in training pipeline:**
```python
# In src/trainer.py - automatically called when resuming
if resume_from:
    quick_verify_checkpoint(resume_from)  # Fast pre-flight check
    loaded_epoch, loaded_step = self.load_checkpoint(resume_from)
```

**What it checks:**
- âœ… Directory exists
- âœ… Required files present (training_state.pth, config.json)
- âœ… Model weights exist (safetensors or bin)
- âœ… Files not empty
- âœ… Training state loadable

**Benefits:**
- Fails fast if checkpoint invalid
- No manual intervention needed
- Minimal overhead (< 1 second)

---

## ğŸ¯ When to Use Each Tool

### Use `verify_checkpoint.py` when:
- âœ… Resuming training after interruption
- âœ… Switching between different machines
- âœ… Checkpoint was saved to Google Drive (may have sync issues)
- âœ… Training failed with checkpoint-related errors
- âœ… Want comprehensive validation before long training run

### Use `validate_checkpoint.py` when:
- âœ… Want to understand checkpoint state
- âœ… Checking if epoch is complete or mid-epoch
- âœ… Debugging resume behavior
- âœ… Verifying expected batches will be trained

### Use `quick_verify_checkpoint()` when:
- âœ… Already integrated (automatic in training)
- âœ… Need fast checks
- âœ… Basic sanity validation sufficient

---

## ğŸ›¡ï¸ Safety Features

### Automatic Integration

All verification improvements are **automatically integrated** into training:

1. **Quick verification** before loading checkpoint
2. **Integrity validation** when loading files
3. **Optimizer consistency check** after loading state
4. **Enhanced scheduler logging** during resume

### No Manual Steps Required

Just resume normally:
```python
trainer.train(
    num_epochs=10,
    resume_from="checkpoints/best_model"  # âœ… All checks run automatically
)
```

---

## ğŸ“Š Understanding Checkpoint States

### End-of-Epoch Checkpoint
```
epoch=2, step=0, total_batches=1270
â†’ Status: COMPLETED
â†’ Resume: Start epoch 3
â†’ Batches trained: All 1270 batches of epoch 2
```

### Mid-Epoch Checkpoint
```
epoch=3, step=500, total_batches=1270
â†’ Status: INCOMPLETE (39.4% done)
â†’ Resume: Continue epoch 3 from step 500
â†’ Will skip: Batches 0-499
â†’ Will train: Batches 500-1269
```

---

## âš ï¸ Common Warnings and What They Mean

### 1. "Optimizer steps differ from expected"
```
âš ï¸  WARNING: Optimizer steps (2532) differ from expected (2540) by 8 steps
```
**Meaning:** Small mismatch due to gradient accumulation  
**Action:** Usually harmless if diff < 20 steps

### 2. "Field 'scaler_state_dict' missing"
```
âš ï¸  WARNING: Field 'scaler_state_dict' missing
```
**Meaning:** Legacy checkpoint without AMP state  
**Action:** AMP may restart from scratch (minor impact)

### 3. "Learning rate very small after fast-forward"
```
âš ï¸  WARNING: Learning rate very small after fast-forward!
```
**Meaning:** Scheduler has decayed significantly  
**Action:** Verify num_epochs and warmup settings

---

## ğŸ”§ Advanced Usage

### Strict Mode

Treat all warnings as errors:
```bash
python scripts/verify_checkpoint.py \
    --checkpoint_path /path/to/checkpoint \
    --strict
```

Use when:
- Critical production training
- Ensuring perfect checkpoint state
- Debugging subtle issues

### Automation

Integrate into CI/CD:
```bash
# Exit code 0 = success, 1 = failure
python scripts/verify_checkpoint.py --checkpoint_path $CKPT_PATH
if [ $? -eq 0 ]; then
    echo "Checkpoint valid, starting training"
    python train.py --resume_from $CKPT_PATH
else
    echo "Checkpoint invalid, aborting"
    exit 1
fi
```

---

## ğŸ“š Documentation

- **Full Guide:** `docs/checkpoint_improvements.md`
- **Architecture:** See IMPROVEMENT 1-5 in source code
- **Examples:** See test outputs in this README

---

## ğŸ“ Best Practices

1. **Always verify before resuming long training runs**
   ```bash
   python scripts/verify_checkpoint.py --checkpoint_path /path/to/checkpoint
   ```

2. **Check both `best_model` and `last_checkpoint`**
   ```bash
   # Verify best model
   python scripts/verify_checkpoint.py --checkpoint_path checkpoints/best_model
   
   # Verify latest checkpoint
   python scripts/verify_checkpoint.py --checkpoint_path checkpoints/last_checkpoint
   ```

3. **Use strict mode for critical checkpoints**
   ```bash
   python scripts/verify_checkpoint.py --checkpoint_path /path/to/checkpoint --strict
   ```

4. **Review warnings before resuming**
   - Small optimizer mismatches are usually fine
   - Missing scaler_state_dict is minor
   - Large mismatches (> 100 steps) need investigation

---

## ğŸ› Troubleshooting

### "Checkpoint directory does not exist"
- Check path is correct
- Verify Google Drive is mounted (if using Colab)
- Ensure checkpoint save completed successfully

### "Required file missing"
- Checkpoint may be corrupted
- Save was interrupted
- Use previous checkpoint

### "Failed to load training state"
- File corrupted
- Incompatible PyTorch version
- Try loading with `map_location='cpu'`

---

## ğŸ“ Support

For issues or questions:
1. Check `docs/checkpoint_improvements.md`
2. Review training logs
3. Run verification with `--strict` flag
4. Check AGENTS.md for project guidelines

---

**Last Updated:** 2026-01-18  
**Version:** 1.0.0
