{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnStack: Google Colab Deployment\n",
    "\n",
    "This notebook automates the setup and execution of the EnStack project on Google Colab to reproduce the paper's results.\n",
    "\n",
    "### üöÄ Automated Pipeline Features:\n",
    "1. **Environment Setup:** Installs all necessary deep learning libraries.\n",
    "2. **Auto Data Setup:** Downloads and processes the Draper VDISC dataset (~1GB) directly to your Drive.\n",
    "3. **Ensemble Training:** Fine-tunes CodeBERT, GraphCodeBERT, and UniXcoder.\n",
    "4. **Meta-Classifier Comparison:** Reproduces Table III from the paper by comparing SVM, LR, RF, and XGBoost.\n",
    "\n",
    "**‚ö†Ô∏è Note:** Ensure you have enough Google Drive space (~5GB for data and checkpoints)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "print(\"üìÇ Connecting to Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify Drive connection\n",
    "if os.path.exists('/content/drive/MyDrive'):\n",
    "    print(\"‚úÖ Google Drive connected successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to connect to Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository\n",
    "Choose **Public** if your repo is open, or **Private** if you need a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# @markdown ### Repository Settings\n",
    "REPO_TYPE = \"Public\" # @param [\"Public\", \"Private\"]\n",
    "USERNAME = \"TCTri205\" # @param {type:\"string\"}\n",
    "REPO_NAME = \"EnStack-paper\" # @param {type:\"string\"}\n",
    "\n",
    "# Construct URL\n",
    "if REPO_TYPE == \"Public\":\n",
    "    REPO_URL = f\"https://github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "else:\n",
    "    print(\"üîë Enter your Personal Access Token (PAT):\")\n",
    "    token = getpass()\n",
    "    REPO_URL = f\"https://{token}@github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# Clone\n",
    "%cd /content\n",
    "if not os.path.exists(REPO_NAME):\n",
    "    print(f\"‚¨áÔ∏è Cloning {REPO_NAME}...\")\n",
    "    !git clone {REPO_URL}\n",
    "else:\n",
    "    print(\"üîÑ Repository exists. Pulling latest changes...\")\n",
    "    !cd {REPO_NAME} && git pull\n",
    "\n",
    "# Change directory to project root\n",
    "%cd /content/{REPO_NAME}\n",
    "print(f\"‚úÖ Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"üîç Checking GPU availability...\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected. Training will be VERY slow on CPU.\")\n",
    "    print(\"\\n‚ö†Ô∏è  IMPORTANT: Enable GPU for faster training:\")\n",
    "    print(\"   1. Go to Runtime ‚Üí Change runtime type\")\n",
    "    print(\"   2. Select Hardware accelerator: T4 GPU\")\n",
    "    print(\"   3. Click Save and restart the notebook\\n\")\n",
    "    \n",
    "    # Ask user if they want to continue\n",
    "    import time\n",
    "    print(\"‚è≥ Waiting 10 seconds... Press 'Stop' button if you want to enable GPU first.\")\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Install Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Installing dependencies...\")\n",
    "!pip install -r requirements.txt -q\n",
    "\n",
    "# Install additional essential packages for Draper data processing and LLMs\n",
    "!pip install pyyaml tqdm scikit-learn transformers torch h5py tables -q\n",
    "\n",
    "print(\"‚úÖ Environment setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DANGER ZONE: Cleanup Old Sessions\n",
    "If you previously ran this notebook with **DUMMY DATA**, you must clear the old checkpoints and processed files before training with real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ### Clear Previous Data/Checkpoints\n",
    "# @markdown ‚ö†Ô∏è **Warning**: This will delete existing processed .pkl files and model checkpoints on your Drive.\n",
    "CLEAR_CHECKPOINTS = False # @param {type:\"boolean\"}\n",
    "CLEAR_PROCESSED_DATA = False # @param {type:\"boolean\"}\n",
    "\n",
    "BASE_PATH = \"/content/drive/MyDrive/EnStack_Data\"\n",
    "\n",
    "if CLEAR_CHECKPOINTS:\n",
    "    checkpoint_path = f\"{BASE_PATH}/checkpoints\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"üóëÔ∏è  Deleting checkpoints at {checkpoint_path}...\")\n",
    "        import shutil\n",
    "        shutil.rmtree(checkpoint_path)\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è No checkpoints found to delete.\")\n",
    "\n",
    "if CLEAR_PROCESSED_DATA:\n",
    "    print(f\"üóëÔ∏è  Deleting old processed .pkl files...\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        f = f\"{BASE_PATH}/{split}_processed.pkl\"\n",
    "        if os.path.exists(f):\n",
    "            os.remove(f)\n",
    "            print(f\"  Removed: {f}\")\n",
    "\n",
    "print(\"‚úÖ Cleanup check complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download & Prepare Real Data (Draper VDISC)\n",
    "This step automatically downloads and sets up the Draper VDISC dataset required to reproduce the paper's results.\n",
    "\n",
    "**Note:** The dataset files (~1GB total) will be downloaded directly from the official OSF repository to your Google Drive (`EnStack_Data/raw_data/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ### Data Setup\n",
    "# @markdown Run this cell to DOWNLOAD and PROCESS the Draper VDISC dataset.\n",
    "\n",
    "!chmod +x scripts/setup_draper.sh\n",
    "!./scripts/setup_draper.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify Data Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Alternative: Use Dummy Data (For testing code only)\n",
    "If you don't have the Draper dataset yet and just want to test if the code runs, use the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ### Dummy Data Configuration\n",
    "USE_DUMMY_DATA = False # @param {type:\"boolean\"}\n",
    "SAMPLE_SIZE = 1000 # @param {type:\"integer\"}\n",
    "\n",
    "if USE_DUMMY_DATA:\n",
    "    print(f\"üîÑ Preparing DUMMY data (Sample size: {SAMPLE_SIZE})...\")\n",
    "    !python scripts/prepare_data.py --output_dir /content/drive/MyDrive/EnStack_Data --mode synthetic --sample {SAMPLE_SIZE}\n",
    "    print(\"\\n‚úÖ Dummy data ready.\")\n",
    "else:\n",
    "    print(\"Skipping dummy data generation. Using Draper data from previous step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ### Dummy Data Configuration\n",
    "USE_DUMMY_DATA = False # @param {type:\"boolean\"}\n",
    "SAMPLE_SIZE = 1000 # @param {type:\"integer\"}\n",
    "\n",
    "if USE_DUMMY_DATA:\n",
    "    print(f\"üîÑ Preparing DUMMY data (Sample size: {SAMPLE_SIZE})...\")\n",
    "    !python scripts/prepare_data.py --output_dir /content/drive/MyDrive/EnStack_Data --mode synthetic --sample {SAMPLE_SIZE}\n",
    "    print(\"\\n‚úÖ Dummy data ready.\")\n",
    "else:\n",
    "    print(\"Skipping dummy data generation. Using Draper data from previous step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify Data Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "CONFIG_PATH = \"configs/config.yaml\"\n",
    "\n",
    "# Load config\n",
    "if os.path.exists(CONFIG_PATH):\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    data_root = config['data']['root_dir']\n",
    "    print(f\"üîç Configured data path: {data_root}\")\n",
    "    \n",
    "    if os.path.exists(data_root):\n",
    "        print(\"‚úÖ Data directory found on Drive!\")\n",
    "        print(\"   Files:\", os.listdir(data_root))\n",
    "    else:\n",
    "        print(f\"‚ùå Directory '{data_root}' not found.\")\n",
    "        print(\"‚ö†Ô∏è Please ensure you created 'EnStack_Data' in MyDrive and uploaded your .pkl files.\")\n",
    "else:\n",
    "    print(\"‚ùå config.yaml not found. Did the repo clone correctly?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ### Training Configuration\n",
    "# @markdown ‚ö†Ô∏è **For Paper Reproduction**: Keep EPOCHS=10, BATCH_SIZE=16 (as in paper Table I)\n",
    "# @markdown üìù **For Quick Testing**: Reduce EPOCHS to 2-3\n",
    "\n",
    "EPOCHS = 10 # @param {type:\"integer\"}\n",
    "BATCH_SIZE = 16 # @param {type:\"integer\"}\n",
    "RESUME = True # @param {type:\"boolean\"}\n",
    "\n",
    "print(f\"üìã Training will use: {EPOCHS} epochs, batch size {BATCH_SIZE}\")\n",
    "print(f\"‚è±Ô∏è  Estimated time:\")\n",
    "print(f\"   - Per epoch on GPU: ~5-10 minutes\")\n",
    "print(f\"   - Total ({EPOCHS} epochs): ~{EPOCHS * 7} minutes on GPU\")\n",
    "print(f\"\\nüí° Tip: For quick testing, set EPOCHS=2 (results won't match paper)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Training Pipeline & Stacking\n",
    "This cell trains the base models (CodeBERT, GraphCodeBERT, UniXcoder) and then trains the meta-classifier (default: SVM).\n",
    "\n",
    "**Note:** To reproduce the paper's comparison between different meta-classifiers (Table III), we will run a comparison cell below after this primary training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main training script\n",
    "cmd = f\"python scripts/train.py --config configs/config.yaml --epochs {EPOCHS} --batch-size {BATCH_SIZE}\"\n",
    "if RESUME:\n",
    "    cmd += \" --resume\"\n",
    "\n",
    "get_ipython().system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Meta-Classifier Comparison (Reproduction of Table III)\n",
    "The EnStack paper compares several meta-classifiers: **Logistic Regression (LR), Random Forest (RF), Support Vector Machine (SVM), and XGBoost**.\n",
    "\n",
    "The primary training script above ran the default (SVM). The cell below will evaluate all of them on the extracted features to reproduce the comparison table from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure project root is in path for imports\n",
    "if str(Path('.').resolve()) not in sys.path:\n",
    "    sys.path.append(str(Path('.').resolve()))\n",
    "\n",
    "from src.stacking import train_meta_classifier, evaluate_meta_classifier, prepare_meta_features\n",
    "from scripts.train import train_base_models, extract_all_features\n",
    "from src.utils import get_device\n",
    "\n",
    "def reproduce_table_iii():\n",
    "    # Load config\n",
    "    with open(\"configs/config.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    output_dir = Path(config['training']['output_dir'])\n",
    "    data_root = Path(config['data']['root_dir'])\n",
    "    \n",
    "    # Step 1: Prepare labels\n",
    "    print(\"üîç Loading labels...\")\n",
    "    try:\n",
    "        train_df = pd.read_pickle(data_root / config['data']['train_file'])\n",
    "        val_df = pd.read_pickle(data_root / config['data']['val_file'])\n",
    "        test_df = pd.read_pickle(data_root / config['data']['test_file'])\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Data files not found. Did you run the Data Setup step?\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    train_labels = train_df['target'].values\n",
    "    test_labels = test_df['target'].values\n",
    "    \n",
    "    # Step 2: Load models and extract features\n",
    "    print(\"üìä Loading base models and extracting features (Logits)...\")\n",
    "    \n",
    "    # This will load weights from checkpoints (resume=True) without training (epochs=0)\n",
    "    device = get_device()\n",
    "    model_names = config['model']['base_models']\n",
    "    \n",
    "    # Suppress training logs for clarity\n",
    "    trainers = train_base_models(config, model_names, num_epochs=0, device=device, resume=True)\n",
    "    \n",
    "    # Extract features using 'logits' mode as per paper\n",
    "    features_dict = extract_all_features(config, trainers, mode=\"logits\")\n",
    "    \n",
    "    print(\"üß† Preparing meta-features...\")\n",
    "    train_meta_features, _ = prepare_meta_features(features_dict['train'])\n",
    "    test_meta_features, _ = prepare_meta_features(features_dict['test'])\n",
    "    \n",
    "    # Step 3: Train and evaluate each meta-classifier\n",
    "    classifiers = [\"lr\", \"rf\", \"svm\", \"xgboost\"]\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{'Meta-Classifier':<20} | {'Acc (%)':<10} | {'F1 (%)':<10} | {'AUC (%)':<10}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    from tqdm.notebook import tqdm
    
    for cls_type in tqdm(classifiers, desc="Training Meta-Classifiers"):\n",
    "        print(f\"Training {cls_type.upper()}...\", end=\"\\r\")\n",
    "        params = config['model']['meta_classifier_params'].get(cls_type, {})\n",
    "        \n",
    "        # Train\n",
    "        model = train_meta_classifier(train_meta_features, train_labels, classifier_type=cls_type, **params)\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = evaluate_meta_classifier(model, test_meta_features, test_labels)\n",
    "        \n",
    "        results.append({\n",
    "            \"Classifier\": cls_type.upper(),\n",
    "            \"Accuracy\": metrics['accuracy'] * 100,\n",
    "            \"F1\": metrics['f1'] * 100,\n",
    "            \"AUC\": metrics['auc'] * 100\n",
    "        })\n",
    "        \n",
    "        print(f\"{cls_type.upper():<20} | {results[-1]['Accuracy']:<10.2f} | {results[-1]['F1']:<10.2f} | {results[-1]['AUC']:<10.2f}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display as DataFrame\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    comparison_df = reproduce_table_iii()\n",
    "    display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion & Discussion\n",
    "Compare the results above with **Table III** in the paper:\n",
    "\n",
    "| Model | Accuracy (%) | F1-Score (%) | AUC-Score (%) |\n",
    "| :--- | :---: | :---: | :---: |\n",
    "| Ensemble Stacking G+U (SVM) | 82.36 | 82.28 | 90.53 |\n",
    "| Ensemble Stacking G+U (LR) | 82.36 | 82.21 | 92.85 |\n",
    "\n",
    "**Note:** Differences in results might occur due to random sampling in downsampling or slight differences in hardware (Tesla P100 in paper vs T4 in Colab)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
