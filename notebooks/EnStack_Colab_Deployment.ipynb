{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnStack: Advanced Stacking Ensemble for Vulnerability Detection\n",
    "\n",
    "This notebook provides a professional, fully optimized pipeline for reproducing the results of the EnStack paper on Google Colab.\n",
    "\n",
    "### ‚ö° Optimized Features:\n",
    "1.  **High-Speed Training:** Automatic Mixed Precision (AMP) and Dynamic Padding (+5-8x speed).\n",
    "2.  **Memory Efficient:** Lazy Loading and Gradient Checkpointing (Run large LLMs on T4 GPU).\n",
    "3.  **Algorithmic Correctness:** K-Fold Out-of-Fold (OOF) stacking to prevent data leakage.\n",
    "4.  **Advanced Visualization:** Confusion matrices, ROC curves, and Feature Importance plots.\n",
    "5.  **Production Ready:** Export models to ONNX and TorchScript.\n",
    "6.  **Robust Checkpoint System:** Atomic saves, resume capability, crash recovery.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "# 1. Mount Drive\n",
    "print(\"üìÇ Connecting to Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Clone Repository\n",
    "REPO_NAME = \"EnStack-paper\" # @param {type:\"string\"}\n",
    "GITHUB_USER = \"TCTri205\" # @param {type:\"string\"}\n",
    "\n",
    "%cd /content\n",
    "if not os.path.exists(REPO_NAME):\n",
    "    print(f\"‚¨áÔ∏è Cloning {REPO_NAME}...\")\n",
    "    !git clone https://github.com/{GITHUB_USER}/{REPO_NAME}.git\n",
    "else:\n",
    "    print(\"üîÑ Repository exists. Pulling latest optimized version...\")\n",
    "    !cd {REPO_NAME} && git pull\n",
    "\n",
    "%cd /content/{REPO_NAME}\n",
    "\n",
    "# 3. Install Dependencies\n",
    "print(\"üì¶ Installing high-performance dependencies...\")\n",
    "!pip install -r requirements.txt -q\n",
    "!pip install transformers[torch] datasets pyarrow xgboost tensorboard seaborn matplotlib -q\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete. Ready to train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Hardware Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import torch\n",
    "\n",
    "print(\"üîç Hardware Check:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå GPU NOT FOUND. Please go to: Runtime -> Change runtime type -> T4 GPU\")\n",
    "\n",
    "print(f\"‚úÖ System RAM: {psutil.virtual_memory().total / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Workflow Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# @markdown ### ‚öôÔ∏è Execution Mode\n",
    "# @markdown Select **\"Fresh Start\"** to wipe old data/models and start over.<br>\n",
    "# @markdown Select **\"Resume Training\"** to continue from the last checkpoint.\n",
    "EXECUTION_MODE = \"Resume Training\" # @param [\"Fresh Start\", \"Resume Training\"]\n",
    "\n",
    "DATA_DIR = \"/content/drive/MyDrive/EnStack_Data\"\n",
    "CHECKPOINT_DIR = f\"{DATA_DIR}/checkpoints\"\n",
    "\n",
    "if EXECUTION_MODE == \"Fresh Start\":\n",
    "    print(f\"‚ö†Ô∏è Fresh Start selected. Cleaning up {DATA_DIR}...\")\n",
    "\n",
    "    # Delete checkpoints\n",
    "    if os.path.exists(CHECKPOINT_DIR):\n",
    "        shutil.rmtree(CHECKPOINT_DIR)\n",
    "        print(f\"   - Deleted checkpoints: {CHECKPOINT_DIR}\")\n",
    "\n",
    "    # Delete cache files\n",
    "    if os.path.exists(DATA_DIR):\n",
    "        for f in os.listdir(DATA_DIR):\n",
    "            if f.startswith(\".cache_\") or f.endswith(\"_processed.pkl\"):\n",
    "                file_path = os.path.join(DATA_DIR, f)\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"   - Deleted: {f}\")\n",
    "\n",
    "    RESUME_TRAINING = False\n",
    "    print(\"‚úÖ Cleanup complete. Ready for fresh training.\")\n",
    "\n",
    "else:\n",
    "    print(\"üîÑ Resume Mode selected.\")\n",
    "    if os.path.exists(CHECKPOINT_DIR):\n",
    "        # Check for last_checkpoint, best_model, or any intermediate checkpoints\n",
    "        all_models = [\"codebert\", \"graphcodebert\", \"unixcoder\"]\n",
    "        for model_name in all_models:\n",
    "            model_dir = os.path.join(CHECKPOINT_DIR, model_name)\n",
    "            if os.path.exists(model_dir):\n",
    "                last_ckpt = os.path.join(model_dir, \"last_checkpoint\")\n",
    "                best_model = os.path.join(model_dir, \"best_model\")\n",
    "                \n",
    "                if os.path.exists(last_ckpt):\n",
    "                    print(f\"   ‚úÖ {model_name}: Found last_checkpoint\")\n",
    "                elif os.path.exists(best_model):\n",
    "                    print(f\"   ‚úÖ {model_name}: Found best_model\")\n",
    "                else:\n",
    "                    # Check for intermediate checkpoints\n",
    "                    import glob\n",
    "                    intermediates = glob.glob(os.path.join(model_dir, \"checkpoint_epoch*_step*\"))\n",
    "                    if intermediates:\n",
    "                        latest = sorted(intermediates)[-1]\n",
    "                        print(f\"   ‚ö†Ô∏è  {model_name}: Found intermediate checkpoint: {os.path.basename(latest)}\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ùå {model_name}: No valid checkpoints found\")\n",
    "\n",
    "        RESUME_TRAINING = True\n",
    "    else:\n",
    "        print(\"   - No checkpoints found. Will start training from scratch.\")\n",
    "        RESUME_TRAINING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. (Optional) Validate Checkpoint State\n",
    "\n",
    "Run this cell to check the current checkpoint state before resuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ### üîç Checkpoint Validation (Optional)\n",
    "# @markdown Run this to verify checkpoint state before training\n",
    "RUN_VALIDATION = False # @param {type:\"boolean\"}\n",
    "\n",
    "if RUN_VALIDATION and os.path.exists(CHECKPOINT_DIR):\n",
    "    print(\"üîç Validating checkpoint state...\\n\")\n",
    "\n",
    "    # Import validation function\n",
    "    import sys\n",
    "    sys.path.append('/content/EnStack-paper')\n",
    "    from scripts.train import find_latest_checkpoint\n",
    "    from pathlib import Path\n",
    "    \n",
    "    all_models = [\"codebert\", \"graphcodebert\", \"unixcoder\"]\n",
    "    for model_name in all_models:\n",
    "        model_dir = os.path.join(CHECKPOINT_DIR, model_name)\n",
    "        if os.path.exists(model_dir):\n",
    "            # Use the same logic as training to find latest checkpoint\n",
    "            latest_ckpt = find_latest_checkpoint(Path(model_dir))\n",
    "            if latest_ckpt:\n",
    "                print(f\"\\n{'='*70}\")\n",
    "                print(f\"Validating {model_name}: {os.path.basename(latest_ckpt)}\")\n",
    "                print('='*70)\n",
    "                !python scripts/validate_checkpoint.py --checkpoint_path {latest_ckpt}\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  {model_name}: No checkpoint to validate\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping validation (not enabled or no checkpoints)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "Choose to use the **Full Draper VDISC** dataset (paper reproduction) or **Dummy Data** (quick code test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ### Data Source Configuration\n",
    "DATA_MODE = \"Draper VDISC\" # @param [\"Draper VDISC\", \"Dummy Data\"]\n",
    "SAMPLE_SIZE = 5000 # @param {type:\"integer\"}\n",
    "\n",
    "if DATA_MODE == \"Draper VDISC\":\n",
    "    print(\"üöÄ Downloading and processing Draper VDISC (~1GB)...\")\n",
    "    !chmod +x scripts/setup_draper.sh\n",
    "    !./scripts/setup_draper.sh\n",
    "else:\n",
    "    print(f\"üîÑ Generating synthetic dummy data ({SAMPLE_SIZE} samples)...\")\n",
    "    !python scripts/prepare_data.py --output_dir /content/drive/MyDrive/EnStack_Data --mode synthetic --sample {SAMPLE_SIZE}\n",
    "\n",
    "print(\"\\n‚úÖ Data is ready on Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Selection\n",
    "\n",
    "Choose which base models to train. You can train all models or select specific ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ### ü§ñ Model Selection\n",
    "# @markdown Select which models to train (you can choose one or multiple)\n",
    "TRAIN_CODEBERT = True # @param {type:\"boolean\"}\n",
    "TRAIN_GRAPHCODEBERT = True # @param {type:\"boolean\"}\n",
    "TRAIN_UNIXCODER = True # @param {type:\"boolean\"}\n",
    "\n",
    "# Build model list based on selection\n",
    "SELECTED_MODELS = []\n",
    "if TRAIN_CODEBERT:\n",
    "    SELECTED_MODELS.append('codebert')\n",
    "if TRAIN_GRAPHCODEBERT:\n",
    "    SELECTED_MODELS.append('graphcodebert')\n",
    "if TRAIN_UNIXCODER:\n",
    "    SELECTED_MODELS.append('unixcoder')\n",
    "\n",
    "if not SELECTED_MODELS:\n",
    "    print(\"‚ö†Ô∏è  WARNING: No models selected! Please select at least one model.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Selected models for training: {', '.join(SELECTED_MODELS)}\")\n",
    "    print(f\"   Total: {len(SELECTED_MODELS)} model(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Training Tips:\n",
    "\n",
    "**Training Individual Models:**\n",
    "- Select only ONE model checkbox above to train it individually\n",
    "- This is useful for:\n",
    "  - Re-training a specific model that failed\n",
    "  - Testing with different hyperparameters\n",
    "  - Saving time when you only need certain models\n",
    "\n",
    "**Training All Models:**\n",
    "- Check all three checkboxes to train the complete ensemble\n",
    "- Required for final meta-classifier training and evaluation\n",
    "\n",
    "**Resume Training:**\n",
    "- The system automatically detects existing checkpoints\n",
    "- Will resume from the last saved state for selected models\n",
    "- Unselected models will be skipped even if they have checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Configuration\n",
    "\n",
    "Configure training hyperparameters and checkpoint strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# @markdown ### üéõÔ∏è Training Hyperparameters\n",
    "EPOCHS = 10 # @param {type:\"integer\"}\n",
    "BATCH_SIZE = 16 # @param {type:\"integer\"}\n",
    "ACCUMULATION_STEPS = 1 # @param {type:\"integer\"}\n",
    "# @markdown **SWA (Stochastic Weight Averaging) - Recommended for best results**\n",
    "# @markdown Adds ~1-2 min per epoch but improves F1 by 0.5-1.0%\n",
    "USE_SWA = True # @param {type:\"boolean\"}\n",
    "SWA_START_EPOCH = 6 # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown ### üíæ Checkpoint Strategy\n",
    "# @markdown - `save_steps=0`: Only save at end of epoch (fastest, risky if crash)\n",
    "# @markdown - `save_steps=500`: Save every 500 batches (recommended for Colab)\n",
    "# @markdown - `save_steps=1000`: Less frequent saves (faster, but more wasted work if crash)\n",
    "SAVE_STEPS = 500 # @param {type:\"integer\"}\n",
    "\n",
    "# Update config.yaml with notebook parameters\n",
    "with open('configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['training']['epochs'] = EPOCHS\n",
    "config['training']['batch_size'] = BATCH_SIZE\n",
    "config['training']['gradient_accumulation_steps'] = ACCUMULATION_STEPS\n",
    "config['training']['use_swa'] = USE_SWA\n",
    "config['training']['swa_start'] = SWA_START_EPOCH\n",
    "config['training']['save_steps'] = SAVE_STEPS\n",
    "\n",
    "# Update base_models in config with selected models\n",
    "config['model']['base_models'] = SELECTED_MODELS\n",
    "\n",
    "with open('configs/config.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"‚úÖ Configuration updated:\")\n",
    "print(f\"   - Selected Models: {', '.join(SELECTED_MODELS)}\")\n",
    "print(f\"   - Epochs: {EPOCHS}\")\n",
    "print(f\"   - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   - SWA (Stochastic Weight Averaging): {USE_SWA}\")\n",
    "if USE_SWA:\n",
    "    print(f\"   - SWA Start Epoch: {SWA_START_EPOCH} (will average epochs {SWA_START_EPOCH}-{EPOCHS})\")\n",
    "print(f\"   - Checkpoint Strategy: save every {SAVE_STEPS} steps\" if SAVE_STEPS > 0 else \"   - Checkpoint Strategy: only at end of epoch\")\n",
    "print(f\"   - Resume: {RESUME_TRAINING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Optimized Training Pipeline\n",
    "\n",
    "This cell executes training for your selected base models.\n",
    "\n",
    "**Note:** Training will automatically:\n",
    "- Train only the models you selected above\n",
    "- Resume from last checkpoint if `RESUME_TRAINING=True`\n",
    "- Save checkpoints according to `SAVE_STEPS` strategy\n",
    "- Log progress with detailed checkpoint information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify models are selected\n",
    "if not SELECTED_MODELS:\n",
    "    print(\"‚ùå ERROR: No models selected for training!\")\n",
    "    print(\"   Please go back to 'Model Selection' cell and select at least one model.\")\n",
    "else:\n",
    "    print(\"üöÄ Starting Training Pipeline...\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training models: {', '.join(SELECTED_MODELS)}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Use RESUME_TRAINING variable from Workflow Configuration step\n",
    "    !python scripts/train.py --config configs/config.yaml {'--resume' if RESUME_TRAINING else ''}\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ Training pipeline completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ### üõ°Ô∏è Safety Verification\n",
    "# @markdown Checks if all selected models were successfully trained before proceeding.\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "print(\"üîç Verifying training completeness...\")\n",
    "\n",
    "# Load current configuration\n",
    "with open('configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "required_models = config['model']['base_models']\n",
    "output_dir = config['training']['output_dir']\n",
    "missing_models = []\n",
    "\n",
    "for model in required_models:\n",
    "    # Check for valid checkpoint artifacts\n",
    "    model_dir = os.path.join(output_dir, model)\n",
    "    is_valid = False\n",
    "    \n",
    "    if os.path.exists(model_dir):\n",
    "        # Check for standard HuggingFace weights or our training state\n",
    "        if os.path.exists(os.path.join(model_dir, \"pytorch_model.bin\")) or \\\n",
    "           os.path.exists(os.path.join(model_dir, \"training_state.pth\")):\n",
    "            is_valid = True\n",
    "            \n",
    "    if not is_valid:\n",
    "        missing_models.append(model)\n",
    "\n",
    "if missing_models:\n",
    "    error_msg = (\n",
    "        f\"\\n‚ùå BLOCKING EXECUTION: Missing trained checkpoints for: {', '.join(missing_models)}\\n\"\n",
    "        f\"   The subsequent evaluation steps require these models to be available.\\n\"\n",
    "        f\"   Please check the training logs in Cell 7 for errors.\"\n",
    "    )\n",
    "    raise RuntimeError(error_msg)\n",
    "\n",
    "print(f\"‚úÖ Verification Passed: All {len(required_models)} models are ready for stacking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5. (Optional) Cleanup Old Checkpoints\n",
    "\n",
    "Free up disk space by removing old mid-epoch checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ### üßπ Checkpoint Cleanup\n",
    "# @markdown Remove old mid-epoch checkpoints to save Google Drive space\n",
    "RUN_CLEANUP = False # @param {type:\"boolean\"}\n",
    "KEEP_LAST_N = 0 # @param {type:\"integer\"}\n",
    "\n",
    "if RUN_CLEANUP:\n",
    "    print(\"üßπ Cleaning up old checkpoints...\\n\")\n",
    "\n",
    "    all_models = [\"codebert\", \"graphcodebert\", \"unixcoder\"]\n",
    "    for model_name in all_models:\n",
    "        ckpt_dir = os.path.join(CHECKPOINT_DIR, model_name)\n",
    "        if os.path.exists(ckpt_dir):\n",
    "            print(f\"\\nCleaning {model_name}:\")\n",
    "            !python scripts/cleanup_checkpoints.py \\\n",
    "                --checkpoint_dir {ckpt_dir} \\\n",
    "                --keep-last {KEEP_LAST_N} \\\n",
    "                --auto\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping cleanup (not enabled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Meta-Classifier Comparison (Table III Reproduction)\n",
    "Evaluate different meta-classifiers (SVM, Logistic Regression, XGBoost) on the same optimized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from IPython.display import display\n",
    "\n",
    "from scripts.train import extract_all_features, load_labels_from_file, train_base_models\n",
    "from src.stacking import (\n",
    "    evaluate_meta_classifier,\n",
    "    prepare_meta_features,\n",
    "    train_meta_classifier,\n",
    ")\n",
    "from src.utils import get_device\n",
    "from src.visualization import plot_meta_feature_importance\n",
    "\n",
    "\n",
    "def reproduce_table_iii():\n",
    "    print(\"üìä Comparing Meta-Classifiers (LR vs RF vs SVM vs XGBoost)...\")\n",
    "\n",
    "    with open(\"configs/config.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    device = get_device()\n",
    "    root_dir = Path(config['data']['root_dir'])\n",
    "\n",
    "    # 1. Load models and pre-created dataloaders\n",
    "    trainers, dataloaders = train_base_models(config, config['model']['base_models'],\n",
    "                                             num_epochs=0, device=device, resume=True)\n",
    "\n",
    "    # 2. Extract Optimized Features (with caching)\n",
    "    features_dict = extract_all_features(config, trainers, dataloaders, mode=\"logits\", use_cache=True)\n",
    "\n",
    "    # 3. Load Labels\n",
    "    train_labels = load_labels_from_file(root_dir / config['data']['train_file'])\n",
    "    test_labels = load_labels_from_file(root_dir / config['data']['test_file'])\n",
    "\n",
    "    # 4. Prepare Meta-features with Scaling/PCA\n",
    "    train_meta, _, pca, scaler = prepare_meta_features(features_dict['train'], train_labels, use_pca=True, use_scaling=True)\n",
    "    test_meta, _, _, _ = prepare_meta_features(features_dict['test'], pca_model=pca, scaler=scaler, use_pca=True, use_scaling=True)\n",
    "\n",
    "    # 5. Iterative Evaluation\n",
    "    results = []\n",
    "    for m_type in [\"lr\", \"rf\", \"svm\", \"xgboost\"]:\n",
    "        print(f\"  > Training {m_type.upper()}...\")\n",
    "        params = config['model']['meta_classifier_params'].get(m_type, {})\n",
    "        clf = train_meta_classifier(train_meta, train_labels, classifier_type=m_type, **params)\n",
    "        metrics = evaluate_meta_classifier(clf, test_meta, test_labels)\n",
    "        \n",
    "        if m_type == 'xgboost':\n",
    "            feature_names = []\n",
    "            for model_name in config['model']['base_models']:\n",
    "                num_classes = config['model'].get('num_labels', 5)\n",
    "                for c in range(num_classes):\n",
    "                    feature_names.append(f\"{model_name}_prob_{c}\")\n",
    "            \n",
    "            plot_meta_feature_importance(clf, feature_names, save_path=f\"{config['training']['output_dir']}/feature_importance.png\")\n",
    "        \n",
    "        results.append({\"Classifier\": m_type.upper(), \"Acc\": metrics['accuracy']*100, \"F1\": metrics['f1']*100, \"AUC\": metrics['auc']*100})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "comparison_df = reproduce_table_iii()\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "print(\"üìà Training Curves:\")\n",
    "hist_plots = glob.glob(f\"{config['training']['output_dir']}/**/training_history.png\", recursive=True)\n",
    "for p in hist_plots:\n",
    "    print(f\"Source: {p}\")\n",
    "    display(Image(filename=p))\n",
    "\n",
    "print(\"\\nüéØ Final Confusion Matrix:\")\n",
    "display(Image(filename=f\"{config['training']['output_dir']}/confusion_matrix.png\"))\n",
    "\n",
    "print(\"\\n‚≠ê Feature Importance (Base Model Impact):\")\n",
    "display(Image(filename=f\"{config['training']['output_dir']}/feature_importance.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Export for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the primary model to ONNX for 3x faster CPU inference\n",
    "from src.models import create_model\n",
    "\n",
    "print(\"üöÄ Exporting model for production...\")\n",
    "model_name = config['model']['base_models'][0]\n",
    "model, _ = create_model(model_name, config, pretrained=False)\n",
    "\n",
    "# Find the best checkpoint to export\n",
    "import sys\n",
    "sys.path.append('/content/EnStack-paper')\n",
    "from scripts.train import find_latest_checkpoint\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path(config['training']['output_dir']) / model_name\n",
    "checkpoint_path = find_latest_checkpoint(model_dir)\n",
    "\n",
    "if checkpoint_path:\n",
    "    print(f\"üì¶ Using checkpoint: {os.path.basename(checkpoint_path)}\")\n",
    "    # Load weights from your best run\n",
    "    model.load_state_dict(torch.load(f\"{checkpoint_path}/pytorch_model.bin\", map_location='cpu'), strict=False)\n",
    "\n",
    "    onnx_path = f\"{config['training']['output_dir']}/optimized_model.onnx\"\n",
    "    model.export_onnx(onnx_path)\n",
    "    print(f\"‚úÖ Successfully exported to: {onnx_path}\")\n",
    "else:\n",
    "    print(\"‚ùå No checkpoint found to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Troubleshooting & Documentation\n",
    "\n",
    "Access helper documentation and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìö Available Documentation:\")\n",
    "print(\"\\n1. Checkpoint System:\")\n",
    "print(\"   - CHECKPOINT_ANALYSIS.md - Root cause analysis\")\n",
    "print(\"   - CHECKPOINT_CORRECTNESS.md - Semantic correctness proof\")\n",
    "print(\"   - CHECKPOINT_VISUAL_GUIDE.md - Visual diagrams\")\n",
    "print(\"   - CHECKPOINT_STRATEGY.md - Configuration guide\")\n",
    "print(\"   - FINAL_VALIDATION.md - Validation summary\")\n",
    "\n",
    "print(\"\\n2. Validation Tools:\")\n",
    "print(\"   !python scripts/validate_checkpoint.py --checkpoint_path <path>\")\n",
    "print(\"   !python scripts/debug_checkpoint.py --checkpoint_path <path>\")\n",
    "print(\"   !python scripts/demo_checkpoint_crash.py\")\n",
    "\n",
    "print(\"\\n3. Cleanup Tools:\")\n",
    "print(\"   !python scripts/cleanup_checkpoints.py --checkpoint_dir <path> --keep-last 0\")\n",
    "\n",
    "print(\"\\n4. Fix Tools:\")\n",
    "print(\"   !python scripts/fix_checkpoint_epoch.py --checkpoint_path <path> --epoch <n>\")\n",
    "\n",
    "print(\"\\nüìñ For detailed guides, check the markdown files in the repository root.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}