{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnStack: Stacking Ensemble for Vulnerability Detection\n",
    "\n",
    "This notebook demonstrates the complete pipeline for training and evaluating the EnStack model on Google Colab.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. Setup environment and mount Google Drive\n",
    "2. Clone repository and install dependencies\n",
    "3. Configure paths for Colab environment\n",
    "4. Train base models (CodeBERT, GraphCodeBERT, UniXcoder)\n",
    "5. Extract features from trained models\n",
    "6. Train meta-classifier (Stacking)\n",
    "7. Evaluate ensemble performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if not already cloned)\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/YOUR_USERNAME/EnStack-paper.git\"  # Update with your repo URL\n",
    "REPO_DIR = \"/content/EnStack-paper\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "else:\n",
    "    print(\"Repository already exists. Pulling latest changes...\")\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "# Change to repository directory\n",
    "%cd {REPO_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd() / \"src\"))\n",
    "\n",
    "from src.utils import load_config, setup_logging, set_seed, get_device\n",
    "from src.dataset import create_dataloaders\n",
    "from src.models import create_model\n",
    "from src.trainer import EnStackTrainer\n",
    "from src.stacking import (\n",
    "    prepare_meta_features,\n",
    "    train_meta_classifier,\n",
    "    evaluate_meta_classifier,\n",
    "    save_meta_classifier,\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging(level=logging.INFO)\n",
    "logger.info(\"EnStack pipeline initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config(\"configs/config.yaml\")\n",
    "\n",
    "# Override paths for Colab environment (if needed)\n",
    "# config[\"data\"][\"root_dir\"] = \"/content/drive/MyDrive/EnStack_Data\"\n",
    "# config[\"training\"][\"output_dir\"] = \"/content/drive/MyDrive/EnStack_Data/checkpoints\"\n",
    "\n",
    "# Set random seed\n",
    "set_seed(config[\"training\"][\"seed\"])\n",
    "\n",
    "# Get device\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Configuration loaded. Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base models to train\n",
    "base_model_names = config[\"model\"][\"base_models\"]\n",
    "num_epochs = config[\"training\"][\"epochs\"]\n",
    "\n",
    "# Storage for trained models\n",
    "trained_models = {}\n",
    "trainers = {}\n",
    "\n",
    "for model_name in base_model_names:\n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"Training {model_name.upper()}\")\n",
    "    logger.info(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Create model and tokenizer\n",
    "    model, tokenizer = create_model(model_name, config, pretrained=True)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        config, tokenizer\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = EnStackTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        learning_rate=config[\"training\"][\"learning_rate\"],\n",
    "        device=device,\n",
    "        output_dir=f\"{config['training']['output_dir']}/{model_name}\",\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = trainer.train(num_epochs=num_epochs, save_best=True)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    if test_loader is not None:\n",
    "        test_metrics = trainer.evaluate(test_loader, split_name=\"Test\")\n",
    "        logger.info(f\"{model_name} Test Results: {test_metrics}\")\n",
    "    \n",
    "    # Store\n",
    "    trained_models[model_name] = model\n",
    "    trainers[model_name] = trainer\n",
    "    \n",
    "    logger.info(f\"\\n{model_name} training completed\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Features for Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"FEATURE EXTRACTION FOR STACKING\")\n",
    "logger.info(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Extract features from each base model\n",
    "train_features_list = []\n",
    "val_features_list = []\n",
    "test_features_list = []\n",
    "\n",
    "for model_name in base_model_names:\n",
    "    logger.info(f\"Extracting features from {model_name}...\")\n",
    "    \n",
    "    trainer = trainers[model_name]\n",
    "    tokenizer = trainer.model.base_model.config._name_or_path\n",
    "    \n",
    "    # Recreate dataloaders with the same tokenizer\n",
    "    from transformers import AutoTokenizer\n",
    "    tok = AutoTokenizer.from_pretrained(config[\"model\"][\"model_map\"][model_name])\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(config, tok)\n",
    "    \n",
    "    # Extract features\n",
    "    if train_loader:\n",
    "        train_features = trainer.extract_features(train_loader)\n",
    "        train_features_list.append(train_features)\n",
    "    \n",
    "    if val_loader:\n",
    "        val_features = trainer.extract_features(val_loader)\n",
    "        val_features_list.append(val_features)\n",
    "    \n",
    "    if test_loader:\n",
    "        test_features = trainer.extract_features(test_loader)\n",
    "        test_features_list.append(test_features)\n",
    "\n",
    "logger.info(\"Feature extraction completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Meta-Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels from datasets\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def load_labels(data_path):\n",
    "    \"\"\"Load labels from data file.\"\"\"\n",
    "    with open(data_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    import pandas as pd\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        data = pd.DataFrame(data)\n",
    "    return data[\"target\"].values\n",
    "\n",
    "root_dir = Path(config[\"data\"][\"root_dir\"])\n",
    "train_labels = load_labels(root_dir / config[\"data\"][\"train_file\"])\n",
    "val_labels = load_labels(root_dir / config[\"data\"][\"val_file\"])\n",
    "test_labels = load_labels(root_dir / config[\"data\"][\"test_file\"])\n",
    "\n",
    "# Prepare meta-features\n",
    "train_meta_features, _ = prepare_meta_features(train_features_list, train_labels)\n",
    "val_meta_features, _ = prepare_meta_features(val_features_list, val_labels)\n",
    "test_meta_features, _ = prepare_meta_features(test_features_list, test_labels)\n",
    "\n",
    "logger.info(f\"Train meta-features shape: {train_meta_features.shape}\")\n",
    "logger.info(f\"Val meta-features shape: {val_meta_features.shape}\")\n",
    "logger.info(f\"Test meta-features shape: {test_meta_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Meta-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"TRAINING META-CLASSIFIER\")\n",
    "logger.info(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Get meta-classifier type from config\n",
    "meta_classifier_type = config[\"model\"].get(\"meta_classifier\", \"svm\")\n",
    "\n",
    "# Train meta-classifier\n",
    "meta_classifier = train_meta_classifier(\n",
    "    train_meta_features,\n",
    "    train_labels,\n",
    "    classifier_type=meta_classifier_type,\n",
    "    random_state=config[\"training\"][\"seed\"],\n",
    ")\n",
    "\n",
    "# Save meta-classifier\n",
    "meta_save_path = f\"{config['training']['output_dir']}/meta_classifier.pkl\"\n",
    "save_meta_classifier(meta_classifier, meta_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"ENSEMBLE EVALUATION\")\n",
    "logger.info(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "logger.info(\"Validation Set Results:\")\n",
    "val_metrics = evaluate_meta_classifier(\n",
    "    meta_classifier, val_meta_features, val_labels\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "logger.info(\"\\nTest Set Results:\")\n",
    "test_metrics = evaluate_meta_classifier(\n",
    "    meta_classifier, test_meta_features, test_labels\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nValidation Metrics:\")\n",
    "for key, value in val_metrics.items():\n",
    "    print(f\"  {key.capitalize()}: {value:.4f}\")\n",
    "print(f\"\\nTest Metrics:\")\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"  {key.capitalize()}: {value:.4f}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Get predictions\n",
    "test_predictions = meta_classifier.predict(test_meta_features)\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('EnStack Confusion Matrix (Test Set)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
