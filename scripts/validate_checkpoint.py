"""
Comprehensive checkpoint validation tool.
Checks if checkpoint state matches the actual model weights.

Usage: python scripts/validate_checkpoint.py --checkpoint_path <path>
"""

import argparse
from pathlib import Path

import torch


def validate_checkpoint_consistency(checkpoint_path: str):
    """
    Validate that checkpoint metadata is consistent with model state.
    """
    checkpoint_dir = Path(checkpoint_path)

    print("=" * 70)
    print("CHECKPOINT VALIDATION")
    print("=" * 70)

    # Load training state
    state_file = checkpoint_dir / "training_state.pth"
    if not state_file.exists():
        print(f"‚ùå No training_state.pth found in {checkpoint_dir}")
        return

    state = torch.load(state_file, map_location="cpu")

    epoch = state.get("epoch", "N/A")
    step = state.get("step", "N/A")
    total_batches = state.get("total_batches", "N/A")

    print("\nüìä CHECKPOINT METADATA:")
    print(f"  Epoch: {epoch}")
    print(f"  Step: {step}")
    print(f"  Total Batches: {total_batches}")

    # Interpret what this means
    print("\nüîç INTERPRETATION:")

    if step == 0:
        print("  ‚úÖ This is an END-OF-EPOCH checkpoint")
        print(f"  üìù Meaning: Epoch {epoch} is COMPLETED")
        print("  üì¶ Model has trained on:")
        print(
            f"     - ALL batches of epoch {epoch} (batches 0 to {total_batches - 1 if total_batches != 'N/A' else '?'})"
        )
        print(
            f"  ‚û°Ô∏è  When resuming: Will start epoch {epoch + 1 if epoch != 'N/A' else '?'}"
        )

    elif step != "N/A" and total_batches != "N/A":
        batches_trained = step
        batches_remaining = total_batches - step
        progress = (step / total_batches) * 100 if total_batches > 0 else 0

        print("  ‚è∏Ô∏è  This is a MID-EPOCH checkpoint")
        print(f"  üìù Meaning: Epoch {epoch} is INCOMPLETE")
        print("  üì¶ Model has trained on:")
        print(
            f"     - Batches 0 to {step - 1} of epoch {epoch} ({batches_trained} batches)"
        )
        print("  ‚è≠Ô∏è  NOT YET trained:")
        print(
            f"     - Batches {step} to {total_batches - 1} ({batches_remaining} batches)"
        )
        print(f"  üìà Progress: {progress:.1f}%")
        print(
            f"  ‚û°Ô∏è  When resuming: Will skip batches 0-{step - 1}, train batches {step}-{total_batches - 1}"
        )

        # Important note about wasted work
        if batches_remaining > 100:
            print(f"\n  ‚ö†Ô∏è  WARNING: {batches_remaining} batches remaining!")
            print(
                "     If training was interrupted, you may have already trained some of"
            )
            print(f"     batches {step}-{total_batches - 1} before the crash.")
            print("     Those batches will be RE-TRAINED when resuming.")
            print("     This is EXPECTED behavior with mid-epoch checkpoints.")

    # Check optimizer state
    print("\nüîß OPTIMIZER STATE:")
    if "optimizer_state_dict" in state:
        opt_state = state["optimizer_state_dict"]
        if "state" in opt_state and len(opt_state["state"]) > 0:
            # Get first param state to check
            first_param_state = opt_state["state"][0]
            if "step" in first_param_state:
                opt_steps = first_param_state["step"].item()
                print(f"  ‚úÖ Optimizer has performed {opt_steps} steps")

                # This should match the checkpoint step for mid-epoch
                if step != 0 and step != "N/A":
                    expected_opt_steps = step
                    if (
                        abs(opt_steps - expected_opt_steps) < 10
                    ):  # Allow small difference
                        print(
                            f"  ‚úÖ Optimizer steps ({opt_steps}) matches checkpoint step ({step})"
                        )
                    else:
                        print(
                            f"  ‚ö†Ô∏è  MISMATCH: Optimizer steps ({opt_steps}) != checkpoint step ({step})"
                        )
                        print("     This might indicate an inconsistent checkpoint!")
        else:
            print("  ‚ö†Ô∏è  Optimizer state exists but appears empty")
    else:
        print("  ‚ùå No optimizer state found")

    # Check model files
    print("\nüìÅ MODEL FILES:")
    model_files = ["pytorch_model.bin", "model.safetensors", "config.json"]
    for fname in model_files:
        fpath = checkpoint_dir / fname
        if fpath.exists():
            size_mb = fpath.stat().st_size / (1024 * 1024)
            print(f"  ‚úÖ {fname:<25} ({size_mb:>8.1f} MB)")
        else:
            print(f"  ‚ùå {fname:<25} (missing)")

    # Summary
    print("\n" + "=" * 70)
    print("SUMMARY")
    print("=" * 70)

    if step == 0:
        print(f"‚úÖ This checkpoint represents a COMPLETE epoch {epoch}")
        print(
            f"‚úÖ Safe to resume - will start epoch {epoch + 1 if epoch != 'N/A' else '?'}"
        )
        print("‚úÖ No batches will be skipped or duplicated")
    else:
        if total_batches != "N/A" and step < total_batches:
            wasted_batches = total_batches - step
            wasted_time_min = wasted_batches * 3.22 / 60  # Estimate
            print(f"‚ö†Ô∏è  This checkpoint represents an INCOMPLETE epoch {epoch}")
            print(
                f"‚ö†Ô∏è  {wasted_batches} batches may have been trained AFTER this checkpoint"
            )
            print(f"   (estimated ~{wasted_time_min:.1f} minutes of wasted work)")
            print("‚úÖ When resuming: Will correctly train all remaining batches")
            print("‚úÖ No batches will be permanently skipped")
            print("‚ö†Ô∏è  Some batches may be trained twice (this is expected)")

    print("=" * 70)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Validate checkpoint consistency")
    parser.add_argument(
        "--checkpoint_path",
        type=str,
        required=True,
        help="Path to checkpoint directory",
    )

    args = parser.parse_args()
    validate_checkpoint_consistency(args.checkpoint_path)
