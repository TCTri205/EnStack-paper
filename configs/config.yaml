data:
  root_dir: "/content/drive/MyDrive/EnStack_Data"
  train_file: "train_processed.pkl"
  val_file: "val_processed.pkl"
  test_file: "test_processed.pkl"

model:
  base_models: 
    - "codebert"
    - "graphcodebert"
    - "unixcoder"
  # Mapping names to HuggingFace models
  model_map:
    codebert: "microsoft/codebert-base"
    graphcodebert: "microsoft/graphcodebert-base"
    unixcoder: "microsoft/unixcoder-base"
  meta_classifier: "svm" # Options: lr, svm, rf, xgboost
  num_labels: 5

  # Hyperparameters from Table I and Table III of the paper
  meta_classifier_params:
    lr:
      max_iter: 200
      solver: "liblinear"
    rf:
      n_estimators: 200
      max_depth: 10
    svm:
      kernel: "rbf"
      probability: True
    xgboost:
      n_estimators: 100
      learning_rate: 0.1
      max_depth: 6
      eval_metric: "mlogloss"

training:
  batch_size: 16
  epochs: 10
  learning_rate: 2.0e-5
  max_length: 512
  seed: 42
  output_dir: "/content/drive/MyDrive/EnStack_Data/checkpoints"
  
  # Advanced Training Options
  scheduler: "cosine"        # Options: linear, cosine
  use_amp: True              # Automatic Mixed Precision
  use_swa: False             # Stochastic Weight Averaging
  swa_start: 5
  gradient_accumulation_steps: 1
  early_stopping_patience: 3
  early_stopping_metric: "f1"
  
  # Checkpoint Options
  save_steps: 500            # Save mid-epoch checkpoint every N steps (0 = disable)
  # save_steps: 0 means ONLY save at end of epoch (faster, but risky if crash mid-epoch)
  # save_steps: 500 means save every 500 steps (safer, but slower and uses more disk)
  
  # Data Pipeline Options
  use_dynamic_padding: True
  lazy_loading: False
  cache_tokenization: True
  
  # Stacking Options
  stacking_mode: "logits"     # Options: logits, embedding
  pooling_mode: "mean"        # Options: cls, mean
  use_pca: True
  pca_components: null        # null = 95% variance
  use_scaling: True
